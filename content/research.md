+++
title = "Research"
description = "machine learning, optimization, & compilers"
+++

## A parallelizing compiler for high-speed network packet processing

Advisor: Dr. Srinivas Narayana Ganapathy, Rutgers University

We explore techniques to parallelize packet-processing code across multiple CPU cores, and building compilers to automatically translate single-core code to run efficiently on multi-core CPUs. I use eBPF for packet processing and clang as the compiler frontend.

## Federated graph machine learning for fair resource allocation

Advisor: Dr. Jian Kang, University of Rochester

We investigate a Federated Learning (FL) model to fairly allocate resources across a network. Our goal is to build a model that can perform edge-level tasks both across clients and within client's structured information. This model should simultaneously maximize global utility while minimizing variance across agents.

## Decentralized optimization for learning over networks

Advisor: Dr. Farzad Yousefian, Rutgers University

We propose a modification of the Distributed Stochastic Gradient Tracking (DSGT) algorithm to reduce the cost of hyperparameter tuning during training of distributed models. We introduce the IR-DSGT algorithm, which acts as a heuristic for the optimal constant regularizer during training. We find that IR-DSGT retains the same convergence rate, up to a constant, as a centralized SGD algorithm.
