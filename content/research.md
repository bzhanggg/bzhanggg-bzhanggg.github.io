+++
title = "Research"
description = "optimization & machine learning"
+++

## Decentralized optimization for learning over networks

We are working on using an iterated regularization parameter for the Distributed Stochastic Gradient Tracking (DSGT) algorithm to reduce the cost of hyperparameter tuning during training of distributed models. We introduce the Iterated-Regularized IR-DSGT algorithm, which acts as a heuristic for the optimal constant regularizer during training. We find that IR-DSGT retains the same convergence rate, up to a constant, as a centralized SGD algorithm.

## Federated Graph Machine Learning
