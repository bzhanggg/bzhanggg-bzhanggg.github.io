+++
title = "Research"
description = "machine learning, optimization, & compilers"
+++

## Decentralized optimization for learning over networks

Advisor: Dr. Farzad Yousefian, Rutgers University

We propose a modification of the Distributed Stochastic Gradient Tracking (DSGT) algorithm to reduce the cost of hyperparameter tuning during training of distributed models. We introduce the IR-DSGT algorithm, which acts as a heuristic for the optimal constant regularizer during training. We find that IR-DSGT retains the same convergence rate, up to a constant, as a centralized SGD algorithm.

## Federated graph machine learning

Advisor: Dr. Jian Kang, University of Rochester

## A parallelizing compiler for high-speed network packet processing

Advisor: Dr. Srinivas Narayana Ganapathy, Rutgers University
